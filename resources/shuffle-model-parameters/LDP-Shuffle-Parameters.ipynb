{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLDP - Geolife & Smart Meter dataset\n",
    "### Dataset preprocessing and computing parameters\n",
    "\n",
    "In this notebook, we show how we created the datasets as used in the paper from the original open datasets.\n",
    "Moreover, we compute the value of the $\\gamma$ parameter for our use case evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Privacy Parameters\n",
    "\n",
    "Before going into the specifics of the use cases, we first define the randomizer algorithms for real values and histograms (following the paper).\n",
    "\n",
    "The code below also determines the privacy amplification we get through shuffling, for some arbitrarily chosen parameters. Give the target overall $\\epsilon$ and $\\delta$ and the number of participants $n$, it gives the $\\epsilon_0$ needed by the local randomizer.\n",
    "\n",
    "We make use of the [shuffleddp repository](https://github.com/BorjaBalle/amplification-by-shuffling) [Balle'19] to determine the bounds of our LDP randomizers and implement them in Python.\n",
    "\n",
    "Let's first import all the required packages.\n",
    "\n",
    "[Balle'19] Balle, B., Bell, J., Gascón, A. and Nissim, K., 2019. The privacy blanket of the shuffle model. In Advances in Cryptology–CRYPTO 2019: 39th Annual International Cryptology Conference, Santa Barbara, CA, USA, August 18–22, 2019, Proceedings, Part II 39 (pp. 638-667). Springer International Publishing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import kagglehub\n",
    "from urllib.request import urlretrieve, urlcleanup\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from zipfile import ZipFile\n",
    "import datetime\n",
    "from shuffleddp.mechanisms import *\n",
    "from shuffleddp.amplification_bounds import *"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm for Real Values\n",
    "\n",
    "Given $x \\in [0, 1]$, the following algorithm calculates the sum of $i$ such values by first encoding them with precision $k$ and then applying the LDP and the Analyzer algorithm given in Section 4.1 of [Balle'19]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Target (eps, delta)-guarantee required\n",
    "eps = 0.1\n",
    "delta = 1e-6\n",
    "\n",
    "n = 5000  # number of participants\n",
    "k = 100  # precision level\n",
    "rrk = RRMechanism(k=k + 1)  # we have the range of {0, 1, ..., k + 1}\n",
    "\n",
    "bound_types = [Hoeffding, BennettExact]\n",
    "all_bounds = []\n",
    "for B in bound_types:\n",
    "    all_bounds.append(B(rrk))\n",
    "\n",
    "print(f\"Epsilon: {eps}\", eps)\n",
    "print(f\"Delta: {delta}\")\n",
    "print(f\"Number of participants: {n}\")\n",
    "bounds = {b.get_name(): b.get_eps0(eps, n, delta) for b in all_bounds}\n",
    "print(f\"Bounds: {bounds}\")\n",
    "\n",
    "gamma = rrk.get_gamma()[0]\n",
    "print(f\"Gamma: {gamma}\")\n",
    "\n",
    "\n",
    "# The first part of the randomizer (float encoding as int)\n",
    "def encode(x, k):\n",
    "    p = x * k - np.floor(x * k)\n",
    "    x_enc = np.floor(x * k) + np.random.binomial(1, p)\n",
    "    return x_enc\n",
    "\n",
    "\n",
    "# second part of the randomizer (randomized response)\n",
    "def RRMech(x, gamma, k):\n",
    "    if not np.random.binomial(1, gamma):\n",
    "        return x\n",
    "    else:\n",
    "        return np.random.randint(k + 1)\n",
    "\n",
    "\n",
    "# apply float encoding to random inputs (as example)\n",
    "true_vals = np.random.rand(1, n)\n",
    "encode_v = np.vectorize(encode)\n",
    "enc_true_vals = encode_v(true_vals, k)\n",
    "\n",
    "# apply randomized response\n",
    "RRMech_v = np.vectorize(RRMech)\n",
    "received_vals = RRMech_v(enc_true_vals[0], gamma, k)\n",
    "\n",
    "# compute outpus\n",
    "sample_sum = sum(received_vals)\n",
    "estimate = (sample_sum / k - gamma * n / 2) / (1 - gamma)\n",
    "print(f\"Estimate: {estimate}\")\n",
    "print(f\"Actual: {sum(true_vals[0])}\")\n",
    "print(f\"Received sum divided by k: {sample_sum / k}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm for Histogram\n",
    "\n",
    "Given $x \\in [k]$, the following algorithm calculates the histogram of values by applying the LDP algorithm given in Section 3.1 of [Balle'19]. Note that the values are already integers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Target (eps, delta)-guarantee required\n",
    "eps = 0.2\n",
    "delta = 1e-6\n",
    "\n",
    "n = 1000  # number of participants\n",
    "k = 100  # precision level\n",
    "rrk = RRMechanism(k=k)  # we have the range of {0, 1, ..., k}\n",
    "\n",
    "bound_types = [Hoeffding, BennettExact]\n",
    "all_bounds = []\n",
    "for B in bound_types:\n",
    "    all_bounds.append(B(rrk))\n",
    "\n",
    "print(f\"Epsilon: {eps}\", eps)\n",
    "print(f\"Delta: {delta}\")\n",
    "print(f\"Number of participants: {n}\")\n",
    "bounds = {b.get_name(): b.get_eps0(eps, n, delta) for b in all_bounds}\n",
    "print(f\"Bounds: {bounds}\")\n",
    "\n",
    "gamma = rrk.get_gamma()[0]\n",
    "print(f\"Gamma: {gamma}\")\n",
    "\n",
    "\n",
    "# the randomizer algorithm (randomized response)\n",
    "def RRMechHist(x, gamma, k):\n",
    "    b = np.random.binomial(1, gamma)\n",
    "    if not b:\n",
    "        return x\n",
    "    else:\n",
    "        return np.random.randint(1, k + 1)\n",
    "\n",
    "\n",
    "# generate random inputs\n",
    "true_vals = np.random.choice(np.arange(1, k + 1), n)\n",
    "\n",
    "# apply randomizer\n",
    "RRMechHist_v = np.vectorize(RRMechHist)\n",
    "received_vals = RRMechHist_v(true_vals, gamma, k)\n",
    "\n",
    "# compute outputs\n",
    "_unique, true_counts = np.unique(true_vals, return_counts=True)\n",
    "_unique, est_counts = np.unique(received_vals, return_counts=True)\n",
    "print(f\"Estimate: {est_counts}\")\n",
    "print(f\"Actual: {true_counts}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart Meter Data (Use Case 1)\n",
    "\n",
    "In this Section we describe the dataset preparation and show how we determine the bounds/run a DP example on this dataset.\n",
    "\n",
    "The dataset is an application of the mechanism for summing up real numbers (from which we can obtain the average as well), i.e., Algorithms for Real Values, as mentioned above. The dataset is taken from: https://www.kaggle.com/datasets/jeanmidev/smart-meters-in-london. In particular, the dataset `daily_dataset.csv` is used.\n",
    "\n",
    "Note: to download the original dataset from Kaggle using the code below, one MIGHT (often it is not needed) has to make an account and set up a token following the \"Installation\" and \"Authentication\" sections on https://www.kaggle.com/docs/api. One can then uncomment the specified line below and use the information from the token to login to the Kaggle API."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# NOTE: IF KAGGLE API KEY IS NEEDED UNCOMMENT BELOW\n",
    "# kagglehub.login()\n",
    "\n",
    "dataset_path = kagglehub.dataset_download(\"jeanmidev/smart-meters-in-london\")\n",
    "dataset = \"daily_dataset.csv\"\n",
    "df = pd.read_csv(os.path.join(dataset_path, dataset))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# inspect dataset\n",
    "df.head"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP Parameters and Example Run (Use Case 1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# maximum possible value of energy -- we will normalize using this\n",
    "max_energy = df[\"energy_mean\"].max()\n",
    "print(\"Max energy:\", max_energy)\n",
    "\n",
    "# Total number of households\n",
    "households = df[\"LCLid\"].unique()\n",
    "n = len(households)\n",
    "\n",
    "eps = 0.2  # Target (eps, delta)-guarantee required\n",
    "delta = 1e-6\n",
    "k = 10  # precision level\n",
    "rrk = RRMechanism(k=k + 1)\n",
    "\n",
    "bound_types = [Hoeffding, BennettExact]\n",
    "all_bounds = []\n",
    "for B in bound_types:\n",
    "    all_bounds.append(B(rrk))\n",
    "\n",
    "print(f\"Epsilon: {eps}\", eps)\n",
    "print(f\"Delta: {delta}\")\n",
    "print(f\"Number of participants: {n}\")\n",
    "bounds = {b.get_name(): b.get_eps0(eps, n, delta) for b in all_bounds}\n",
    "print(f\"Bounds: {bounds}\")\n",
    "\n",
    "gamma = rrk.get_gamma()[0]\n",
    "print(f\"Gamma: {gamma}\")\n",
    "\n",
    "num_days = int(1 / eps)  # we will run the mechanism a total of 1/eps times\n",
    "\n",
    "print(\"k:\", k)\n",
    "print(\"n:\", n)\n",
    "print(\"Number of days:\", num_days)\n",
    "print(\"eps0:\", rrk.get_eps0())\n",
    "print(\"================\\n\")\n",
    "\n",
    "last_day = \"2014-02-25\"\n",
    "cur_date = datetime.datetime.strptime(last_day, '%Y-%m-%d').date()\n",
    "delta = datetime.timedelta(days=1)\n",
    "\n",
    "\n",
    "# Normalize energy values within [0, 1]\n",
    "def normalizeVals(vals):\n",
    "    for i in range(len(vals)):\n",
    "        vals[i] = vals[i] / (max_energy)\n",
    "    return vals\n",
    "\n",
    "\n",
    "# also store data for writing to csv\n",
    "energy_vals_for_csv = []\n",
    "\n",
    "# do an example DP run on this data and simultaneously parse the data\n",
    "for i in range(num_days):\n",
    "    day = cur_date.strftime('%Y-%m-%d')\n",
    "    cur_date -= delta\n",
    "    df0 = df[['LCLid', 'day', 'energy_mean']]\n",
    "    df1 = df0[df0['day'] == day]\n",
    "    energy_vals = [0.0 for i in range(len(households))]\n",
    "    for j in range(len(households)):\n",
    "        energy = df1.loc[df1['LCLid'] == households[j], 'energy_mean'].values\n",
    "        if energy.size != 0:\n",
    "            energy_vals[j] = energy[0]\n",
    "    energy_vals = normalizeVals(energy_vals)\n",
    "    true_vals = energy_vals\n",
    "    encode_v = np.vectorize(encode)\n",
    "    enc_true_vals = encode_v(true_vals, k)\n",
    "\n",
    "    RRMech_v = np.vectorize(RRMech)\n",
    "    received_vals = RRMech_v(enc_true_vals, gamma, k)\n",
    "\n",
    "    sample_sum = sum(received_vals)\n",
    "    # This is the de-biasing step in Algorithm 3 of [Balle'19]\n",
    "    estimate = (sample_sum / k - gamma * n / 2) / (1 - gamma)\n",
    "    print(f\"Run {i + 1}:\")\n",
    "    print(f\"Estimate: {estimate / n}\")\n",
    "    print(f\"Actual: {sum(true_vals) / n}\")\n",
    "    print(\"==================\\n\")\n",
    "    energy_vals_for_csv.append(energy_vals)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Extracted Smart Meter Data\n",
    "\n",
    "The following extracts only relevant information into a CSV file. Namely the average energy consumption per household over the days used in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the normalized energy values into a csv file\n",
    "with open(\"energy_data.csv\", \"w\", newline='') as f:\n",
    "    wr = csv.writer(f, delimiter=\",\")\n",
    "    header_row = [\"household\", \"day\", \"average energy\"]\n",
    "    wr.writerow(header_row)\n",
    "    for i in range(len(energy_vals_for_csv)):\n",
    "        vals = energy_vals_for_csv[i]\n",
    "        for j in range(len(vals)):\n",
    "            row = [j, i, vals[j]]\n",
    "            wr.writerow(row)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolife GPS Trajectory Dataset (Use Case 2)\n",
    "\n",
    "In this Section we describe the dataset preparation and show how we determine the bounds/run a DP example on this dataset.\n",
    "\n",
    "Taken from https://www.microsoft.com/en-us/research/publication/geolife-gps-trajectory-dataset-user-guide/. The following extracts the first longitude, latitude entry on a given date from files corresponding to all users. Not all users have date for each day.\n",
    "\n",
    "First we download and unpack the original data. (Note: This can take up to 10-15 minutes, since it's a lot of data to unpack)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "zip_path, _headers = urlretrieve(\n",
    "    \"https://download.microsoft.com/download/F/4/8/F4894AA5-FDBC-481E-9285-D5F8C4C4F039/Geolife%20Trajectories%201.3.zip\")\n",
    "\n",
    "with ZipFile(zip_path, 'r') as zip_file:\n",
    "    zip_file.extractall(os.getcwd())\n",
    "\n",
    "# remove downloaded tmp file\n",
    "urlcleanup()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Latitude, Longitude Reading from All Users One Day at a Time\n",
    "\n",
    "We only need a subselection of the data, so we do that as follows.\n",
    "This code takes the first lat long from each user's file of the first 5 days."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "work_dir = \"Geolife Trajectories 1.3/Data\"\n",
    "users = [\"{:03d}\".format(i) for i in range(182)]\n",
    "sub_dir = \"Trajectory\"\n",
    "\n",
    "latLongs = []\n",
    "days = 5\n",
    "\n",
    "for user in users:\n",
    "    cur_dir = os.path.join(work_dir, user, sub_dir)\n",
    "    files = sorted([filename for filename in os.listdir(cur_dir)])\n",
    "    for day in range(days):\n",
    "        if day < len(files):\n",
    "            cur_file = os.path.join(cur_dir, files[day])\n",
    "            with open(cur_file, 'r') as f:\n",
    "                reader = csv.reader(f, delimiter='|')\n",
    "                rows = list(reader)\n",
    "\n",
    "            flat_rows = itertools.chain.from_iterable(rows)\n",
    "            list_rows = [i.strip().split(',') for i in flat_rows]\n",
    "\n",
    "            df = pd.DataFrame(list_rows[6:])  # first 6 lines are useless\n",
    "\n",
    "            lat = df.iloc[0][0]\n",
    "            long = df.iloc[0][1]\n",
    "            latLongs.append([user, day, lat, long])\n",
    "\n",
    "print(f\"Number of records: {len(latLongs)}\")\n",
    "print(\"Head:\")\n",
    "print(latLongs[:6])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the Reverse Geo API\n",
    "\n",
    "Next we transform the latitudes and longitudes found in the data into postcodes using a geodata lookup.\n",
    "\n",
    "NOTE: please specify the use agent on line 7, for example use your e-mail or the name of the application. This is necessary to follow the conditions of the API used."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: set USER AGENT\n",
    "# initialize Nominatim API \n",
    "geolocator = Nominatim(user_agent=\"PLEASE SPECIFY\")\n",
    "\n",
    "for idx, data in enumerate(latLongs):\n",
    "    print(f\"{idx + 1}/{len(latLongs)}\")\n",
    "    lat = data[2]\n",
    "    long = data[3]\n",
    "    try:\n",
    "        location = geolocator.reverse(lat + \",\" + long, language='en')\n",
    "        address = location.raw['address']\n",
    "        if 'postcode' in address:\n",
    "            data.append(address['postcode'])\n",
    "        else:\n",
    "            data.append(\"None\")\n",
    "    except GeocoderTimedOut as e:\n",
    "        print(\"Error: geocode failed on input %s\" % (lat + \",\" + long))\n",
    "        data.append(\"TimeOut\")\n",
    "print(\"done\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Head:\")\n",
    "print(latLongs[:6])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for CSV\n",
    "\n",
    "In the code below we make our data ready for the use case. First, we condense the list of postcodes to the top 7 most used ones. The other are aggregated under \"all_others\". Then we will the missing entries with the \"all_others\" category as well and write the resulting data to a CSV file.\n",
    "\n",
    "Note: the resulting .csv file might be slightly different from the one we created, the geodata api calls are not always consistent (timeouts may happen), so this could cause some changes. The overall file will be very similar though."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df = pd.DataFrame(latLongs, columns=[\"User\", \"day\", \"lat\", \"long\", \"postcode\"])\n",
    "\n",
    "# reduce the number of postcodes to the top 7, replace rest by \"all_others\"\n",
    "top_postcodes = df.value_counts(subset=\"postcode\").drop(\"None\").nlargest(7).keys()\n",
    "df[\"postcode\"] = df[\"postcode\"].mask(df[\"postcode\"].isin(top_postcodes) == False, \"all_others\")\n",
    "\n",
    "# fill missing data\n",
    "users = [\"{:03d}\".format(i) for i in range(182)]\n",
    "days = 5\n",
    "for day in range(days):\n",
    "    for user in users:\n",
    "        if not any((df[\"User\"] == user) & (df[\"day\"] == day)):\n",
    "            df.loc[len(df)] = [user, day, None, None, \"all_others\"]\n",
    "\n",
    "# Write to CSV\n",
    "df.to_csv(\"geolife-postcodes-condensed-empties.csv\", index=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the final dataset distribution\n",
    "\n",
    "Below we look at the true distribution of the resulting dataset "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.read_csv(\"geolife-postcodes-condensed-empties.csv\")\n",
    "days = df['day'].unique()\n",
    "users = df['User'].unique()\n",
    "counts = {}\n",
    "for day in days:\n",
    "    counts[day] = {}\n",
    "    postcodes = df.loc[df['day'] == day, 'postcode'].unique()\n",
    "    for postcode in postcodes:\n",
    "        counts[day][postcode] = df[(df['day'] == day) & (df['postcode'] == postcode)].shape[0]\n",
    "\n",
    "for day in counts:\n",
    "    c = 0\n",
    "    for k, v in counts[day].items():\n",
    "        print(k, v)\n",
    "        c = c + v\n",
    "    print(f\"This count: {c}\")\n",
    "    print()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP Parameters and Example Run (Use Case 2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Total number of users\n",
    "users = df[\"User\"].unique()\n",
    "n = len(users)\n",
    "postcodes = df[\"postcode\"].unique()\n",
    "k = len(postcodes)  # histogram of postcodes\n",
    "eps = 2  # Target (eps, delta)-guarantee required\n",
    "# over multiple runs the eps add up, e.g., 5 runs => 5*eps\n",
    "delta = 1e-4\n",
    "\n",
    "rrk = RRMechanism(k=k)  # we have the range of {0, 1, ..., k}\n",
    "\n",
    "bound_types = [Hoeffding, BennettExact]\n",
    "all_bounds = []\n",
    "for B in bound_types:\n",
    "    all_bounds.append(B(rrk))\n",
    "\n",
    "print(f\"Epsilon: {eps}\", eps)\n",
    "print(f\"Delta: {delta}\")\n",
    "print(f\"Number of participants: {n}\")\n",
    "bounds = {b.get_name(): b.get_eps0(eps, n, delta) for b in all_bounds}\n",
    "print(f\"Bounds: {bounds}\")\n",
    "\n",
    "gamma = rrk.get_gamma()[0]\n",
    "print(f\"Gamma: {gamma}\")\n",
    "\n",
    "days = df[\"day\"].unique()\n",
    "\n",
    "\n",
    "def RRMech(x, gamma, postcodes):\n",
    "    if not np.random.binomial(1, gamma):\n",
    "        return x\n",
    "    else:\n",
    "        return np.random.choice(postcodes)\n",
    "\n",
    "\n",
    "orig_dict = {}\n",
    "syn_dict = {}\n",
    "\n",
    "# do an example run\n",
    "i = 0\n",
    "for day in days:\n",
    "    i += 1\n",
    "    orig_dict[day] = {}\n",
    "    syn_dict[day] = {}\n",
    "    df0 = df[['User', 'day', 'postcode']]\n",
    "    df1 = df0[df0['day'] == day]\n",
    "    for user in users:\n",
    "        postcode = df1.loc[df1['User'] == user, 'postcode'].values\n",
    "        #print(postcode)\n",
    "        if postcode.size != 0:\n",
    "            postcode = postcode[0]\n",
    "            if postcode in orig_dict[day]:\n",
    "                orig_dict[day][postcode] += 1\n",
    "            else:\n",
    "                orig_dict[day][postcode] = 1\n",
    "            priv_postcode = RRMech(postcode, gamma, postcodes)\n",
    "            if priv_postcode in syn_dict[day]:\n",
    "                syn_dict[day][priv_postcode] += 1\n",
    "            else:\n",
    "                syn_dict[day][priv_postcode] = 1\n",
    "\n",
    "    print(\"Run \" + str(i) + \":\")\n",
    "    print(\"Postcode, DP, Original:\")\n",
    "    print(\"==========\\n\")\n",
    "    for k, v1 in syn_dict[day].items():\n",
    "        v2 = orig_dict[day][k]\n",
    "        print(k + \", \" + str(v1) + \", \" + str(v2))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
